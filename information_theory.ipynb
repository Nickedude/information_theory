{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theroy\n",
    "\n",
    "This notebook contains an introduction to information theory and some simple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Example\n",
    "Which language contains the most information? Assume that we have a language of 4 characters, where each character occurs with some probability. How much information does the language contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1)\n",
    "A: 25%\n",
    "B: 25%\n",
    "C: 25%\n",
    "D: 25%\n",
    "\n",
    "        *\n",
    "       / \\\n",
    "      *   *\n",
    "     / \\ / \\\n",
    "    A  B C  D\n",
    "    \n",
    "We must ask two questions to know the next character in a random string, as visualized above. Therefore, the language contains two bits of information. Another way to see it is that we can encode the language with two bits.\n",
    "A: 00\n",
    "B: 01\n",
    "C: 10\n",
    "D: 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2)\n",
    "A: 50%\n",
    "B: 12.5%\n",
    "C: 12.5%\n",
    "D: 25%\n",
    "\n",
    "        *\n",
    "       / \\\n",
    "      A   *\n",
    "         / \\\n",
    "        D   *\n",
    "           / \\\n",
    "          B   C\n",
    "    \n",
    "Here, we might get the next character after one question! What is the expected number of questions? This is given by the formula:\n",
    "\n",
    "$\\sum_s Pr(s) \\cdot n(s)$\n",
    "\n",
    "where $s$ is a character, $Pr(s)$ the probability of its occurence and $n(s)$ the number of questions we need to ask. In this case we get:\n",
    "\n",
    "$\\frac{1}{2} \\cdot 1 + \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 3 = \\frac{2}{4} + \\frac{2}{4} + \\frac{3}{4} = \\frac{7}{4} =  1.75 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $n(s)$ can be expressed in terms of $Pr(s)$ using $log$. For example, we have that $Pr(A) = \\frac{1}{2}$, and clearly $log(\\frac{1}{\\frac{1}{2}}) = log(2) = 1 = n(A)$. We use 2 as a base since that's the number of branches in the language tree. This allows us to calculate the number of expected bits using only the probabilities:\n",
    "\n",
    "$\\sum_s Pr(s) \\cdot n(s) =  \\sum_s Pr(s) \\cdot log(\\frac{1}{Pr(s)}) = \\sum_s Pr(s) \\cdot (log(1) - log(Pr(s))) = \\sum_s Pr(s) \\cdot - log(Pr(s)) = - \\sum_s Pr(s) \\cdot log(Pr(s))$\n",
    "\n",
    "The number of expected bits is also referred to as the **Entropy** of the language, and is commonly denoted $H(S)$ where $S$ is a random variable, which in this example was a language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** $H(X) = - \\sum_x Pr(x) \\cdot log(Pr(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
