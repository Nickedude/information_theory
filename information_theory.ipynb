{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theroy\n",
    "\n",
    "This notebook contains an introduction to information theory and some simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia:\n",
    "\n",
    "The information entropy, or just entropy, is a basic quantity in information theory associated to any random variable. It can be interpreted as the average level of \"information\" or \"uncertainty\" inherent in the variable's possible outcomes. \n",
    "\n",
    "Intuitively we can think of it like this. A variable with no randomness (100% of a single event) carries no new information, since we know the outcome of all future events. However, variables with more randomness carries more information, since we can't perfectly predict the outcome of all future events for that variable. \n",
    "\n",
    "There is an analogy to physics here. A lot of disorder (i.e randomness) => high entropy, low level of disorder => low entropy.\n",
    "\n",
    "It makes intuitive sense that entropy is maximized by the uniform distribution, since all events are then equally probable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Example\n",
    "As an example, consider a random variable $S$ representing a language. Which language contains the most information? \n",
    "\n",
    "Assume that we have a language of 4 characters, where each character occurs with some probability. How much information does the language contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1)\n",
    "A: 25%\n",
    "B: 25%\n",
    "C: 25%\n",
    "D: 25%\n",
    "\n",
    "        *\n",
    "       / \\\n",
    "      *   *\n",
    "     / \\ / \\\n",
    "    A  B C  D\n",
    "    \n",
    "We must ask two questions to know the next character in a random string, as visualized above. Therefore, the language contains two bits of information. Another way to see it is that we can encode the language with two bits.\n",
    "A: 00\n",
    "B: 01\n",
    "C: 10\n",
    "D: 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2)\n",
    "A: 50%\n",
    "B: 12.5%\n",
    "C: 12.5%\n",
    "D: 25%\n",
    "\n",
    "        *\n",
    "       / \\\n",
    "      A   *\n",
    "         / \\\n",
    "        D   *\n",
    "           / \\\n",
    "          B   C\n",
    "    \n",
    "Here, we might get the next character after one question! What is the expected number of questions? This is given by the formula:\n",
    "\n",
    "$\\sum_s Pr(s) \\cdot n(s)$\n",
    "\n",
    "where $s$ is a character, $Pr(s)$ the probability of its occurence and $n(s)$ the number of questions we need to ask. In this case we get:\n",
    "\n",
    "$\\frac{1}{2} \\cdot 1 + \\frac{1}{4} \\cdot 2 + \\frac{1}{4} \\cdot 3 = \\frac{2}{4} + \\frac{2}{4} + \\frac{3}{4} = \\frac{7}{4} =  1.75 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $n(s)$ can be expressed in terms of $Pr(s)$ using $log$. For example, we have that $Pr(A) = \\frac{1}{2}$, and clearly $log(\\frac{1}{\\frac{1}{2}}) = log(2) = 1 = n(A)$. We use 2 as a base since that's the number of branches in the language tree. This allows us to calculate the number of expected bits using only the probabilities:\n",
    "\n",
    "$\\sum_s Pr(s) \\cdot n(s) =  \\sum_s Pr(s) \\cdot log(\\frac{1}{Pr(s)}) = \\sum_s Pr(s) \\cdot (log(1) - log(Pr(s))) = \\sum_s Pr(s) \\cdot - log(Pr(s)) = - \\sum_s Pr(s) \\cdot log(Pr(s))$\n",
    "\n",
    "The number of expected bits is also referred to as the **Entropy** of the language, and is commonly denoted $H(S)$ where $S$ is a random variable, which in this example was a language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** $H(X) = - \\sum_x Pr(x) \\cdot log(Pr(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probabilities: np.ndarray):\n",
    "    assert len(probabilities.shape) == 1  # Assume 1D array\n",
    "    \n",
    "    return -np.sum(probabilities * np.log2(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of example one: 2.0\n",
      "Entropy of example two: 1.75\n"
     ]
    }
   ],
   "source": [
    "example_one = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "example_two = np.array([0.5, 0.25, 0.125, 0.125])\n",
    "\n",
    "print(f\"Entropy of example one: {entropy(example_one)}\")\n",
    "print(f\"Entropy of example two: {entropy(example_two)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also consider the joint Entropy of two random variables. This is defined as:\n",
    "\n",
    "$H(X, Y) = -\\sum_x \\sum_y Pr(X, Y) \\cdot log(Pr(X, Y))$\n",
    "\n",
    "where $Pr(X,Y)$ is the joint probability. We can also consider the conditional entropy, defined as:\n",
    "\n",
    "$H(X | Y) = -\\sum_x Pr(X | Y) \\cdot log(Pr(X|Y))$\n",
    "\n",
    "If the variables are independent, then it holds that:\n",
    "\n",
    "$H(X | Y) = H(X)$\n",
    "\n",
    "If $H(X | Y)$ is small, then either $H(X)$ is small or $H(X)$ is high, but the knowledge of $Y$ has caused $H(X | Y)$ to become small. To better understand what information is shared between the two variables, we use mutual information:\n",
    "\n",
    "$I(X, Y) = H(Y) - H(X, Y)$\n",
    "\n",
    "Mutual Informatio determines how different the joint distribution is from the product of the marginal distributions. That is, if the joint probability distrubtion is denoted $P_{X,Y}$ and the marginal distributions are $P_X$ and $P_Y$, then \n",
    "\n",
    "$I(X,Y) = D_{KL}(P_{X,Y} || P_X \\cdot P_Y) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](visualization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
